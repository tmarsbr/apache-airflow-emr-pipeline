# DAG ETL Airlines com EMR Spark

Esta DAG processa dados de airlines do S3 usando EMR com Spark e salva o resultado em formato Parquet particionado.

## ğŸ¯ Objetivo

Processar dados do bucket `bkt-athena-class-podacademy-meu/athena/airlines/` e salvar no bucket `bkt-athena-meu/processed/airlines/` em formato Parquet particionado por year/month/day.

## ğŸ“‹ PrÃ©-requisitos

### 1. Roles IAM do EMR

VocÃª precisa criar as seguintes roles IAM:

#### EMR_DefaultRole (Service Role)
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "elasticmapreduce.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
        }
    ]
}
```

Policies anexadas:
- `AmazonElasticMapReduceRole`

#### EMR_EC2_DefaultRole (Instance Profile)
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "ec2.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
        }
    ]
}
```

Policies anexadas:
- `AmazonElasticMapReduceforEC2Role`

### 2. PermissÃµes S3 Adicionais

Adicione estas permissÃµes ao usuÃ¡rio/role que executa o Airflow:

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "elasticmapreduce:*",
                "ec2:DescribeInstances",
                "ec2:DescribeInstanceStatus",
                "iam:ListInstanceProfiles",
                "iam:PassRole"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::bkt-athena-class-podacademy-meu/*",
                "arn:aws:s3:::bkt-athena-meu/*",
                "arn:aws:s3:::bkt-athena-class-podacademy-meu",
                "arn:aws:s3:::bkt-athena-meu"
            ]
        }
    ]
}
```

## ğŸ—ï¸ Arquitetura da DAG

### Tasks:
1. **configurar_credenciais** - Configura credenciais AWS
2. **verificar_dados_origem** - Verifica dados no bucket origem
3. **criar_script_spark** - Cria e uploada script Spark para S3
4. **criar_cluster_emr** - Cria cluster EMR
5. **adicionar_steps_spark** - Adiciona job Spark ao cluster
6. **aguardar_conclusao_step** - Aguarda conclusÃ£o do processamento
7. **terminar_cluster_emr** - Termina o cluster EMR
8. **verificar_resultado_final** - Verifica dados processados

### ConfiguraÃ§Ã£o EMR:
- **Release**: EMR 6.15.0
- **AplicaÃ§Ãµes**: Spark, Hadoop
- **Master**: 1x m5.xlarge
- **Workers**: 2x m5.large
- **Auto-terminaÃ§Ã£o**: Habilitada

## ğŸ“Š Processamento de Dados

### Entrada:
- **Bucket**: `bkt-athena-class-podacademy-meu`
- **Path**: `athena/airlines/`
- **Formato**: CSV (inferido automaticamente)

### SaÃ­da:
- **Bucket**: `bkt-athena-meu`
- **Path**: `processed/airlines/`
- **Formato**: Parquet
- **Particionamento**: year/month/day
- **CompressÃ£o**: Snappy

### Exemplo de estrutura de partiÃ§Ãµes:
```
s3://bkt-athena-meu/processed/airlines/
â”œâ”€â”€ year=2024/
â”‚   â”œâ”€â”€ month=1/
â”‚   â”‚   â”œâ”€â”€ day=1/
â”‚   â”‚   â”‚   â”œâ”€â”€ part-00000.snappy.parquet
â”‚   â”‚   â”‚   â””â”€â”€ part-00001.snappy.parquet
â”‚   â”‚   â””â”€â”€ day=2/
â”‚   â””â”€â”€ month=2/
â””â”€â”€ year=2023/
```

## ğŸš€ Como Executar

### 1. Configurar ConexÃ£o AWS no Airflow

No Airflow UI:
1. Admin â†’ Connections
2. Add new record:
   - **Connection Id**: `aws_default`
   - **Connection Type**: `Amazon Web Services`
   - **Login**: Seu AWS Access Key ID
   - **Password**: Seu AWS Secret Access Key
   - **Extra**: `{"region_name": "sa-east-1"}`

### 2. Executar a DAG

1. Acesse http://localhost:8080
2. Encontre a DAG `etl_airlines_emr_spark`
3. Execute manualmente ou aguarde execuÃ§Ã£o agendada (diÃ¡ria)

## ğŸ“ˆ Monitoramento

### Logs importantes:
- **Cluster EMR**: Logs salvos em `s3://bkt-athena-meu/emr-logs/`
- **Airflow**: Interface web com logs detalhados
- **Spark**: Logs disponÃ­veis na UI do EMR

### MÃ©tricas esperadas:
- **Tempo de execuÃ§Ã£o**: 10-30 minutos (depende do tamanho dos dados)
- **Custo EMR**: ~$2-5 por execuÃ§Ã£o (depende da regiÃ£o e tempo)
- **ReduÃ§Ã£o de tamanho**: 60-80% (CSV â†’ Parquet comprimido)

## ğŸ”§ PersonalizaÃ§Ã£o

### Alterar configuraÃ§Ã£o do cluster:
Edite a variÃ¡vel `EMR_CLUSTER_CONFIG` no arquivo da DAG.

### Modificar processamento Spark:
Edite a funÃ§Ã£o `criar_script_spark()` para ajustar a lÃ³gica de transformaÃ§Ã£o.

### Alterar particionamento:
Modifique o script Spark para usar outras colunas de partiÃ§Ã£o.

## âš ï¸ Cuidados Importantes

1. **Custos**: O EMR gera custos por tempo de execuÃ§Ã£o
2. **Permissions**: Verifique todas as permissÃµes IAM
3. **RegiÃ£o**: Certifique-se que buckets e EMR estÃ£o na mesma regiÃ£o
4. **Cleanup**: A DAG termina automaticamente o cluster apÃ³s uso

## ğŸ› Troubleshooting

### Erro "EMR_DefaultRole does not exist":
- Crie as roles IAM conforme documentado acima

### Erro "Access Denied S3":
- Verifique permissÃµes S3 nas roles EMR

### Timeout no processamento:
- Aumente o timeout na task `aguardar_conclusao_step`
- Considere usar instÃ¢ncias maiores no EMR

### Script Spark falha:
- Verifique logs do EMR
- Ajuste schema/formato dos dados de entrada